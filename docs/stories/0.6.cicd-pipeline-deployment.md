# Story 0.6: CI/CD Pipeline and Deployment Infrastructure

## Status
Ready for Implementation

## Story
**As a** developer,
**I want** automated deployment pipeline and infrastructure configuration,
**so that** code can be deployed safely and consistently to production.

## Acceptance Criteria
1. GitHub Actions workflow configured for automated testing and deployment
2. Environment-specific configuration (dev, staging, production)
3. Database migration automation integrated with deployment
4. Docker production build configuration optimized
5. Infrastructure as Code (IaC) setup for cloud resources

## Technical Requirements
- Automated testing pipeline runs on pull requests
- Production Docker image optimized for performance and security
- Environment variable management for different deployment stages
- Database backup and recovery procedures automated
- Blue-green or rolling deployment strategy implemented
- Health check endpoints for monitoring deployment status

## Detailed CI/CD Pipeline Specification

### GitHub Actions Workflows

#### 1. Pull Request Validation (`.github/workflows/pr-validation.yml`)

```yaml
name: Pull Request Validation

on:
  pull_request:
    branches: [main, develop]
    paths-ignore:
      - 'docs/**'
      - '*.md'
      - '.gitignore'

env:
  NODE_VERSION: '20'
  POSTGRES_VERSION: '18'

jobs:
  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: TypeScript type checking
        run: npm run type-check

      - name: ESLint code analysis
        run: npm run lint

      - name: Prettier format checking
        run: npm run format:check

      - name: Security audit
        run: npm audit --audit-level=moderate

  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run unit tests
        run: npm run test:unit
        env:
          CI: true

      - name: Upload test coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage/lcov.info
          fail_ci_if_error: false

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20

    services:
      postgres:
        image: postgres:18-alpine
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: printmgmt_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

      minio:
        image: minio/minio:latest
        env:
          MINIO_ROOT_USER: minioadmin
          MINIO_ROOT_PASSWORD: minioadmin
        options: >-
          --health-cmd "curl -f http://localhost:9000/minio/health/live"
          --health-interval 30s
          --health-timeout 20s
          --health-retries 3
        ports:
          - 9000:9000
          - 9001:9001

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Setup test database
        run: |
          npm run db:migrate:test
          npm run db:seed:test
        env:
          DATABASE_URL: postgresql://postgres:test_password@localhost:5432/printmgmt_test

      - name: Run integration tests
        run: npm run test:integration
        env:
          DATABASE_URL: postgresql://postgres:test_password@localhost:5432/printmgmt_test
          REDIS_URL: redis://localhost:6379
          MINIO_ENDPOINT: localhost:9000
          MINIO_ACCESS_KEY: minioadmin
          MINIO_SECRET_KEY: minioadmin

  build-validation:
    name: Build Validation
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build application
        run: npm run build

      - name: Validate build artifacts
        run: |
          test -d dist
          test -f dist/index.html
          test -f dist/assets/*.js
          test -f dist/assets/*.css
```

#### 2. Staging Deployment (`.github/workflows/staging-deploy.yml`)

```yaml
name: Staging Deployment

on:
  push:
    branches: [develop]
  workflow_dispatch:

env:
  NODE_VERSION: '20'
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  build-and-push:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    outputs:
      image-digest: ${{ steps.build.outputs.digest }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=sha,prefix={{branch}}-

      - name: Build and push
        id: build
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile.production
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64

  deploy-staging:
    name: Deploy to Staging
    needs: build-and-push
    runs-on: ubuntu-latest
    environment: staging

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ~1.5

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Terraform init
        run: terraform init
        working-directory: ./infrastructure/staging

      - name: Terraform plan
        run: terraform plan -var="app_image=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ needs.build-and-push.outputs.image-digest }}"
        working-directory: ./infrastructure/staging

      - name: Terraform apply
        run: terraform apply -auto-approve -var="app_image=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ needs.build-and-push.outputs.image-digest }}"
        working-directory: ./infrastructure/staging

      - name: Run database migrations
        run: |
          aws ecs run-task \
            --cluster printmgmt-staging \
            --task-definition printmgmt-staging-migrate:ACTIVE \
            --launch-type FARGATE \
            --network-configuration "awsvpcConfiguration={subnets=[${{ secrets.STAGING_SUBNET_IDS }}],securityGroups=[${{ secrets.STAGING_SECURITY_GROUP }}],assignPublicIp=ENABLED}"

      - name: Health check
        run: |
          timeout 300 bash -c 'until curl -f ${{ secrets.STAGING_URL }}/health; do sleep 10; done'

      - name: Run smoke tests
        run: npm run test:smoke
        env:
          TEST_URL: ${{ secrets.STAGING_URL }}
```

#### 3. Production Deployment (`.github/workflows/production-deploy.yml`)

```yaml
name: Production Deployment

on:
  push:
    branches: [main]
  workflow_dispatch:
    inputs:
      skip_tests:
        description: 'Skip test execution'
        required: false
        default: false
        type: boolean

env:
  NODE_VERSION: '20'
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  pre-deployment-tests:
    name: Pre-deployment Tests
    runs-on: ubuntu-latest
    if: ${{ !inputs.skip_tests }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run full test suite
        run: npm run test:all

      - name: Security scan
        run: npm run security:scan

  build-production:
    name: Build Production Image
    needs: pre-deployment-tests
    if: always() && (needs.pre-deployment-tests.result == 'success' || inputs.skip_tests)
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    outputs:
      image-digest: ${{ steps.build.outputs.digest }}
      image-tag: ${{ steps.meta.outputs.tags }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=sha,prefix=prod-
            type=raw,value=latest

      - name: Build and push
        id: build
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile.production
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64

      - name: Sign container image
        run: |
          cosign sign --yes ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ steps.build.outputs.digest }}
        env:
          COSIGN_EXPERIMENTAL: 1

  deploy-production:
    name: Deploy to Production
    needs: build-production
    runs-on: ubuntu-latest
    environment: production

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ~1.5

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: us-east-1

      - name: Create database backup
        run: |
          aws rds create-db-snapshot \
            --db-instance-identifier printmgmt-prod \
            --db-snapshot-identifier printmgmt-backup-$(date +%Y%m%d-%H%M%S)

      - name: Blue-Green Deployment
        run: |
          # Deploy to green environment
          terraform init
          terraform plan -var="app_image=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ needs.build-production.outputs.image-digest }}" -var="deployment_color=green"
          terraform apply -auto-approve -var="app_image=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}@${{ needs.build-production.outputs.image-digest }}" -var="deployment_color=green"
        working-directory: ./infrastructure/production

      - name: Run database migrations
        run: |
          aws ecs run-task \
            --cluster printmgmt-production \
            --task-definition printmgmt-production-migrate:ACTIVE \
            --launch-type FARGATE \
            --network-configuration "awsvpcConfiguration={subnets=[${{ secrets.PRODUCTION_SUBNET_IDS }}],securityGroups=[${{ secrets.PRODUCTION_SECURITY_GROUP }}],assignPublicIp=ENABLED}"

      - name: Health check green environment
        run: |
          timeout 600 bash -c 'until curl -f ${{ secrets.GREEN_URL }}/health; do sleep 15; done'

      - name: Run production smoke tests
        run: npm run test:smoke:production
        env:
          TEST_URL: ${{ secrets.GREEN_URL }}

      - name: Switch traffic to green
        run: |
          terraform apply -auto-approve -var="active_color=green"
        working-directory: ./infrastructure/production

      - name: Final health check
        run: |
          timeout 300 bash -c 'until curl -f ${{ secrets.PRODUCTION_URL }}/health; do sleep 10; done'

      - name: Cleanup old blue environment
        run: |
          sleep 300  # Wait 5 minutes before cleanup
          terraform apply -auto-approve -var="cleanup_blue=true"
        working-directory: ./infrastructure/production

  post-deployment:
    name: Post-deployment Tasks
    needs: deploy-production
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Notify deployment status
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          channel: '#deployments'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}

      - name: Update deployment tracking
        run: |
          curl -X POST "${{ secrets.DEPLOYMENT_TRACKER_URL }}" \
            -H "Authorization: Bearer ${{ secrets.DEPLOYMENT_TOKEN }}" \
            -H "Content-Type: application/json" \
            -d '{
              "environment": "production",
              "version": "${{ github.sha }}",
              "image": "${{ needs.build-production.outputs.image-tag }}",
              "status": "${{ job.status }}",
              "timestamp": "${{ github.event.head_commit.timestamp }}"
            }'
```

### Environment Configuration

#### Environment Variables Structure

**Development (`.env.development`)**
```bash
# Application
NODE_ENV=development
APP_URL=http://localhost:3000
APP_NAME="3D Print Manager (Dev)"

# Database
DATABASE_URL=postgresql://postgres:dev_password@localhost:5432/printmgmt_dev
REDIS_URL=redis://localhost:6379

# File Storage (MinIO for local dev)
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_BUCKET=printmgmt-dev
MINIO_USE_SSL=false

# Authentication (Clerk Dev)
CLERK_PUBLISHABLE_KEY=pk_test_...
CLERK_SECRET_KEY=sk_test_...
CLERK_WEBHOOK_SECRET=whsec_...

# Email (Development - Console logs)
EMAIL_PROVIDER=console
SMTP_HOST=
SMTP_PORT=
SMTP_USER=
SMTP_PASS=

# Monitoring
SENTRY_DSN=
LOG_LEVEL=debug
```

**Staging (AWS Parameter Store paths)**
```bash
# /printmgmt/staging/app/url
# /printmgmt/staging/app/name
# /printmgmt/staging/database/url
# /printmgmt/staging/redis/url
# /printmgmt/staging/cloudflare/r2/access_key
# /printmgmt/staging/cloudflare/r2/secret_key
# /printmgmt/staging/cloudflare/r2/bucket
# /printmgmt/staging/clerk/publishable_key
# /printmgmt/staging/clerk/secret_key
# /printmgmt/staging/sendgrid/api_key
# /printmgmt/staging/sentry/dsn
```

**Production (AWS Parameter Store with encryption)**
```bash
# /printmgmt/production/app/url (SecureString)
# /printmgmt/production/app/name (String)
# /printmgmt/production/database/url (SecureString)
# /printmgmt/production/redis/url (SecureString)
# /printmgmt/production/cloudflare/r2/access_key (SecureString)
# /printmgmt/production/cloudflare/r2/secret_key (SecureString)
# /printmgmt/production/cloudflare/r2/bucket (String)
# /printmgmt/production/clerk/publishable_key (String)
# /printmgmt/production/clerk/secret_key (SecureString)
# /printmgmt/production/sendgrid/api_key (SecureString)
# /printmgmt/production/sentry/dsn (String)
```

### Docker Configuration

#### Development (`docker-compose.yml`)
```yaml
version: '3.8'

services:
  postgres:
    image: postgres:18-alpine
    environment:
      POSTGRES_DB: printmgmt_dev
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: dev_password
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init-db.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  minio:
    image: minio/minio:latest
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

volumes:
  postgres_data:
  redis_data:
  minio_data:
```

#### Production (`Dockerfile.production`)
```dockerfile
# Multi-stage production build
FROM node:20-alpine AS base
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production && npm cache clean --force

FROM node:20-alpine AS build
WORKDIR /app
COPY package*.json ./
RUN npm ci
COPY . .
RUN npm run build

FROM node:20-alpine AS runtime
# Security: Non-root user
RUN addgroup -g 1001 -S nodejs
RUN adduser -S nextjs -u 1001

WORKDIR /app

# Copy production dependencies
COPY --from=base /app/node_modules ./node_modules
COPY --from=build /app/dist ./dist
COPY --from=build /app/package.json ./package.json

# Security: Set correct permissions
RUN chown -R nextjs:nodejs /app
USER nextjs

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \
  CMD curl -f http://localhost:3000/health || exit 1

EXPOSE 3000

# Graceful shutdown support
STOPSIGNAL SIGTERM

CMD ["npm", "start"]
```

### Infrastructure as Code (Terraform)

#### Staging Environment (`infrastructure/staging/main.tf`)
```hcl
terraform {
  required_version = ">= 1.5"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }

  backend "s3" {
    bucket = "printmgmt-terraform-state"
    key    = "staging/terraform.tfstate"
    region = "us-east-1"
  }
}

provider "aws" {
  region = "us-east-1"

  default_tags {
    tags = {
      Environment = "staging"
      Project     = "printmgmt"
      ManagedBy   = "terraform"
    }
  }
}

# VPC and Networking
module "vpc" {
  source = "../modules/vpc"

  name_prefix = "printmgmt-staging"
  cidr_block  = "10.1.0.0/16"

  availability_zones = ["us-east-1a", "us-east-1b"]

  tags = {
    Environment = "staging"
  }
}

# RDS Database
module "database" {
  source = "../modules/rds"

  identifier = "printmgmt-staging"
  engine     = "postgres"
  engine_version = "18.1"

  instance_class = "db.t3.micro"
  allocated_storage = 20
  storage_encrypted = true

  database_name = "printmgmt_staging"
  username      = "printmgmt"

  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnet_ids

  backup_retention_period = 7
  backup_window          = "03:00-04:00"
  maintenance_window     = "sun:04:00-sun:05:00"

  monitoring_interval = 60

  tags = {
    Environment = "staging"
  }
}

# Redis Cache
module "redis" {
  source = "../modules/elasticache"

  cluster_id = "printmgmt-staging"
  node_type  = "cache.t3.micro"

  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnet_ids

  tags = {
    Environment = "staging"
  }
}

# ECS Cluster
module "ecs" {
  source = "../modules/ecs"

  cluster_name = "printmgmt-staging"

  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnet_ids

  app_image = var.app_image
  app_port  = 3000

  cpu    = 256
  memory = 512

  desired_count = 1
  min_capacity  = 1
  max_capacity  = 3

  environment_variables = {
    NODE_ENV = "staging"
    DATABASE_URL = module.database.connection_string
    REDIS_URL = module.redis.connection_string
  }

  tags = {
    Environment = "staging"
  }
}

# Application Load Balancer
module "alb" {
  source = "../modules/alb"

  name = "printmgmt-staging"

  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.public_subnet_ids

  target_group_arn = module.ecs.target_group_arn

  certificate_arn = aws_acm_certificate.staging.arn

  tags = {
    Environment = "staging"
  }
}

# SSL Certificate
resource "aws_acm_certificate" "staging" {
  domain_name       = "staging.printmgmt.example.com"
  validation_method = "DNS"

  lifecycle {
    create_before_destroy = true
  }

  tags = {
    Environment = "staging"
  }
}

# Route 53 DNS
resource "aws_route53_record" "staging" {
  zone_id = data.aws_route53_zone.main.zone_id
  name    = "staging.printmgmt.example.com"
  type    = "A"

  alias {
    name                   = module.alb.dns_name
    zone_id                = module.alb.zone_id
    evaluate_target_health = true
  }
}

data "aws_route53_zone" "main" {
  name = "example.com"
}

# Variables
variable "app_image" {
  description = "Docker image for the application"
  type        = string
}

# Outputs
output "database_endpoint" {
  value = module.database.endpoint
}

output "redis_endpoint" {
  value = module.redis.endpoint
}

output "application_url" {
  value = "https://staging.printmgmt.example.com"
}
```

### Database Migration Automation

#### Migration Script (`scripts/migrate.sh`)
```bash
#!/bin/bash

set -euo pipefail

# Configuration
ENVIRONMENT=${1:-development}
DRY_RUN=${2:-false}

echo "ðŸš€ Starting database migration for environment: $ENVIRONMENT"

# Load environment-specific configuration
case $ENVIRONMENT in
  "development")
    DATABASE_URL=${DATABASE_URL:-"postgresql://postgres:dev_password@localhost:5432/printmgmt_dev"}
    ;;
  "staging")
    DATABASE_URL=$(aws ssm get-parameter --name "/printmgmt/staging/database/url" --with-decryption --query "Parameter.Value" --output text)
    ;;
  "production")
    DATABASE_URL=$(aws ssm get-parameter --name "/printmgmt/production/database/url" --with-decryption --query "Parameter.Value" --output text)

    # Production safety check
    if [[ "$DRY_RUN" != "true" ]]; then
      echo "âš ï¸  WARNING: Running migrations against PRODUCTION database!"
      echo "Current time: $(date)"
      echo "Database: $DATABASE_URL"
      echo ""
      echo "Type 'CONFIRM_PRODUCTION_MIGRATION' to proceed:"
      read -r confirmation
      if [[ "$confirmation" != "CONFIRM_PRODUCTION_MIGRATION" ]]; then
        echo "âŒ Migration cancelled by user"
        exit 1
      fi
    fi
    ;;
  *)
    echo "âŒ Unknown environment: $ENVIRONMENT"
    exit 1
    ;;
esac

# Backup before migration (production only)
if [[ "$ENVIRONMENT" == "production" && "$DRY_RUN" != "true" ]]; then
  echo "ðŸ“¦ Creating database backup..."
  BACKUP_NAME="printmgmt-backup-$(date +%Y%m%d-%H%M%S)"
  aws rds create-db-snapshot \
    --db-instance-identifier printmgmt-prod \
    --db-snapshot-identifier "$BACKUP_NAME"

  echo "âœ… Backup created: $BACKUP_NAME"
fi

# Run Prisma migrations
echo "ðŸ”„ Running database migrations..."

if [[ "$DRY_RUN" == "true" ]]; then
  echo "ðŸ” DRY RUN MODE - No changes will be made"
  npx prisma migrate diff --from-migrations ./prisma/migrations --to-schema-datamodel ./prisma/schema.prisma
else
  # Apply migrations
  npx prisma migrate deploy

  # Generate Prisma client
  npx prisma generate

  # Validate schema
  npx prisma validate
fi

echo "âœ… Database migration completed successfully!"

# Health check
echo "ðŸ¥ Running post-migration health check..."
npx tsx scripts/health-check.ts --database-only

echo "ðŸŽ‰ Migration process completed successfully for $ENVIRONMENT environment!"
```

### Health Check Endpoints

#### Health Check Implementation (`src/api/health.ts`)
```typescript
import { createAPIFileRoute } from '@tanstack/start/api'
import { prisma } from '@/lib/db'
import { redis } from '@/lib/redis'

export const Route = createAPIFileRoute('/api/health')({
  GET: async ({ request }) => {
    const url = new URL(request.url)
    const detailed = url.searchParams.get('detailed') === 'true'

    const checks = {
      timestamp: new Date().toISOString(),
      environment: process.env.NODE_ENV,
      version: process.env.APP_VERSION || 'unknown',
      status: 'healthy' as 'healthy' | 'degraded' | 'unhealthy',
      services: {} as Record<string, any>
    }

    try {
      // Database health check
      const dbStart = Date.now()
      await prisma.$queryRaw`SELECT 1`
      const dbLatency = Date.now() - dbStart

      checks.services.database = {
        status: 'healthy',
        latency: `${dbLatency}ms`,
        ...(detailed && {
          connection_pool: await prisma.$queryRaw`
            SELECT
              numbackends as active_connections,
              xact_commit as committed_transactions,
              xact_rollback as rolled_back_transactions
            FROM pg_stat_database
            WHERE datname = current_database()
          `
        })
      }
    } catch (error) {
      checks.services.database = {
        status: 'unhealthy',
        error: error instanceof Error ? error.message : 'Unknown error'
      }
      checks.status = 'unhealthy'
    }

    try {
      // Redis health check
      const redisStart = Date.now()
      await redis.ping()
      const redisLatency = Date.now() - redisStart

      checks.services.redis = {
        status: 'healthy',
        latency: `${redisLatency}ms`,
        ...(detailed && {
          memory_usage: await redis.memory('usage'),
          connected_clients: await redis.client('list')
        })
      }
    } catch (error) {
      checks.services.redis = {
        status: 'unhealthy',
        error: error instanceof Error ? error.message : 'Unknown error'
      }
      if (checks.status === 'healthy') {
        checks.status = 'degraded'
      }
    }

    // File storage health check (Cloudflare R2)
    try {
      const r2Start = Date.now()
      // Simple head request to verify R2 connectivity
      const response = await fetch(`${process.env.R2_ENDPOINT}/${process.env.R2_BUCKET}`, {
        method: 'HEAD',
        headers: {
          'Authorization': `AWS4-HMAC-SHA256 ...` // Proper AWS signature
        }
      })
      const r2Latency = Date.now() - r2Start

      checks.services.file_storage = {
        status: response.ok ? 'healthy' : 'degraded',
        latency: `${r2Latency}ms`
      }
    } catch (error) {
      checks.services.file_storage = {
        status: 'unhealthy',
        error: error instanceof Error ? error.message : 'Unknown error'
      }
      if (checks.status === 'healthy') {
        checks.status = 'degraded'
      }
    }

    const statusCode = checks.status === 'healthy' ? 200 :
                       checks.status === 'degraded' ? 200 : 503

    return new Response(JSON.stringify(checks, null, 2), {
      status: statusCode,
      headers: {
        'Content-Type': 'application/json',
        'Cache-Control': 'no-cache'
      }
    })
  }
})
```

### Monitoring and Alerting

#### CloudWatch Alarms (`infrastructure/modules/monitoring/main.tf`)
```hcl
# Application Health Alarms
resource "aws_cloudwatch_metric_alarm" "app_health_check" {
  alarm_name          = "${var.environment}-app-health-check"
  comparison_operator = "LessThanThreshold"
  evaluation_periods  = "2"
  metric_name         = "HealthCheckStatus"
  namespace           = "AWS/ApplicationELB"
  period              = "60"
  statistic           = "Average"
  threshold           = "1"
  alarm_description   = "This metric monitors application health"
  alarm_actions       = [aws_sns_topic.alerts.arn]

  dimensions = {
    LoadBalancer = var.load_balancer_arn_suffix
  }
}

# Database Connection Alarms
resource "aws_cloudwatch_metric_alarm" "database_connections" {
  alarm_name          = "${var.environment}-database-connections"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "2"
  metric_name         = "DatabaseConnections"
  namespace           = "AWS/RDS"
  period              = "300"
  statistic           = "Average"
  threshold           = "80"
  alarm_description   = "This metric monitors database connection count"
  alarm_actions       = [aws_sns_topic.alerts.arn]

  dimensions = {
    DBInstanceIdentifier = var.db_instance_identifier
  }
}

# Memory Utilization Alarms
resource "aws_cloudwatch_metric_alarm" "memory_utilization" {
  alarm_name          = "${var.environment}-memory-utilization"
  comparison_operator = "GreaterThanThreshold"
  evaluation_periods  = "2"
  metric_name         = "MemoryUtilization"
  namespace           = "AWS/ECS"
  period              = "300"
  statistic           = "Average"
  threshold           = "85"
  alarm_description   = "This metric monitors memory utilization"
  alarm_actions       = [aws_sns_topic.alerts.arn]

  dimensions = {
    ServiceName = var.ecs_service_name
    ClusterName = var.ecs_cluster_name
  }
}

# SNS Topic for Alerts
resource "aws_sns_topic" "alerts" {
  name = "${var.environment}-printmgmt-alerts"
}

resource "aws_sns_topic_subscription" "email_alerts" {
  topic_arn = aws_sns_topic.alerts.arn
  protocol  = "email"
  endpoint  = var.alert_email
}

# Slack Integration
resource "aws_sns_topic_subscription" "slack_alerts" {
  topic_arn = aws_sns_topic.alerts.arn
  protocol  = "https"
  endpoint  = var.slack_webhook_url
}
```

## Deployment Strategy

### Blue-Green Deployment Process

1. **Green Environment Setup**: Deploy new version to green environment
2. **Health Checks**: Verify green environment health
3. **Database Migrations**: Run migrations against shared database
4. **Smoke Tests**: Execute critical path tests
5. **Traffic Switch**: Route traffic from blue to green
6. **Monitoring**: Monitor for 5 minutes post-switch
7. **Cleanup**: Terminate blue environment

### Rollback Procedures

1. **Immediate Rollback**: Switch traffic back to blue environment
2. **Database Rollback**: Use database backup if schema changes required
3. **Image Rollback**: Redeploy previous container image
4. **Configuration Rollback**: Revert environment variables via Parameter Store

## Tasks / Subtasks

- [ ] Create GitHub Actions workflows (AC: 1)
  - [ ] Implement PR validation workflow
  - [ ] Implement staging deployment workflow
  - [ ] Implement production deployment workflow
  - [ ] Configure workflow permissions and secrets
- [ ] Setup environment configurations (AC: 2)
  - [ ] Create development environment configuration
  - [ ] Setup staging AWS Parameter Store configuration
  - [ ] Setup production AWS Parameter Store with encryption
  - [ ] Configure environment-specific secrets management
- [ ] Implement database migration automation (AC: 3)
  - [ ] Create migration scripts with safety checks
  - [ ] Integrate Prisma migrations with deployment
  - [ ] Setup automated backup procedures
  - [ ] Create rollback procedures
- [ ] Configure Docker production build (AC: 4)
  - [ ] Create optimized Dockerfile.production
  - [ ] Setup multi-stage build process
  - [ ] Implement security hardening (non-root user, health checks)
  - [ ] Configure container signing with cosign
- [ ] Implement Infrastructure as Code (AC: 5)
  - [ ] Create Terraform modules for VPC, RDS, ECS, ALB
  - [ ] Setup blue-green deployment infrastructure
  - [ ] Configure monitoring and alerting
  - [ ] Implement cost optimization strategies

## Dependencies

- **Epic 0 Stories 0.1-0.5**: Basic project setup and dependencies must be completed
- **External Services**: AWS account setup, GitHub repository configuration
- **DNS Configuration**: Domain registration and Route 53 setup
- **Security**: SSL certificates, container signing keys, secrets management

## Definition of Done

- [ ] All GitHub Actions workflows execute successfully
- [ ] Blue-green deployment process tested in staging
- [ ] Health check endpoints return proper status codes
- [ ] Database migrations run automatically with rollback capability
- [ ] Monitoring and alerting configured for all environments
- [ ] Security scanning integrated into CI/CD pipeline
- [ ] Documentation updated with deployment procedures